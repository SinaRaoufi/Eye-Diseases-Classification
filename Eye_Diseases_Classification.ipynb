{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad8279f-8d68-426e-b863-b71bdcb718ee",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Eye Diseases Classification üëÅÔ∏è</h1>\n",
    "<p style=\"text-align:center;\">The dataset consists of Normal, Diabetic Retinopathy, Cataract and Glaucoma retinal images where each class have approximately 1000 images. These images are collected from various sorces like IDRiD, Oculur recognition, HRF etc.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bc4280-b0b2-43bf-8dbd-e97565aef5df",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d734402e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MulticlassAccuracy' from 'torchmetrics.classification' (/Users/sinaraoufi/miniforge3/lib/python3.10/site-packages/torchmetrics/classification/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_grid\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accuracy\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MulticlassAccuracy\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrix\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_recall\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MulticlassAccuracy' from 'torchmetrics.classification' (/Users/sinaraoufi/miniforge3/lib/python3.10/site-packages/torchmetrics/classification/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from torchmetrics.functional import precision_recall\n",
    "from torchmetrics.functional import f1_score\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from functools import partial\n",
    "from ray.air import session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588c36f-627e-475f-9325-ef655ce96709",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f51a2d-b715-49f7-8f16-7807e736063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    t = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        ])\n",
    "    \n",
    "    return datasets.ImageFolder(root='dataset', transform=t)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a87b9b-fadf-49af-9fb8-595f0642b3fe",
   "metadata": {},
   "source": [
    "## Split data into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c9ac0-c17b-47de-a562-792a53e5c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, train_size, random_state=42):\n",
    "    train_size = int(train_size * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    seed = torch.Generator().manual_seed(random_state)\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size], generator=seed)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021174f9-162d-4823-99a4-4fa67d8a245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc6856-01ca-4ee0-94a2-e9e0885f653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a1cfc-8d1d-4e9b-ba31-deb9db3be569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(dataset, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf63817-1d14-4158-8a97-d96fe767f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes\n",
    "K = len(set(dataset.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf747b-48b7-4015-8f9b-f293ed54b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of classes: {K}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9db6c-1c15-4705-ad48-b300b9155d93",
   "metadata": {},
   "source": [
    "## Convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f932c7-810d-4582-9406-10f9170bc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, K):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64 * 3 * 3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, K)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dense_layers(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b93e72-1cd3-4c89-b36c-eb2ba66a72c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = CNN(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834496f-fbf5-4fb6-a545-907c894a4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b388c9-b42a-4f92-9037-fba253b89115",
   "metadata": {},
   "source": [
    "### Exploring images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3deb7-c627-4ef0-8380-39aaafbf3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image, label):\n",
    "    print(f\"Label : {dataset.classes[label]}\")\n",
    "    plt.imshow(image.permute(1,2,0))\n",
    "\n",
    "# display the first image in the dataset\n",
    "display_image(*dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4bfc3-d4b1-4fce-853e-e42c05735347",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98d521-c0ad-4ceb-b9a5-e3b56a8a0bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=1)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d405a-d558-45b8-9122-8a7f95015bcd",
   "metadata": {},
   "source": [
    "### Visualizing the batch images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36f4cb-6591-4790-8ddd-ae28cefbca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(data_loader):\n",
    "    '''Plot images grid of single batch'''\n",
    "    for images, labels in data_loader:\n",
    "        fig, ax = plt.subplots(figsize = (16,12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "        break\n",
    "        \n",
    "show_batch(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12b52e-6304-413c-a199-0416544c2a96",
   "metadata": {},
   "source": [
    "### Device configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d588a-84a7-4e0d-b8f4-8a2a59444269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179002c4-b72a-4fd3-89b8-7c75f739e4a0",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b0122-aff8-4393-9d2e-d7684383df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to encapsulate the training loop\n",
    "def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):\n",
    "    \n",
    "    model.to(device)\n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "    \n",
    "    accuracy = MulticlassAccuracy().to(device)\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = []\n",
    "        for inputs, targets in train_loader:\n",
    "            # move data to GPU\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        # Get train loss\n",
    "        train_loss = np.mean(train_loss)\n",
    "        # Train accuracy\n",
    "        train_accuracy = accuracy(outputs, targets)\n",
    "        \n",
    "        test_loss = []\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss.append(loss.item())\n",
    "        \n",
    "        # Get test loss\n",
    "        test_loss = np.mean(test_loss) \n",
    "        # Test accuracy\n",
    "        test_accuracy = accuracy(outputs, targets)\n",
    "        \n",
    "        # Save losses\n",
    "        train_losses[epoch] = train_loss\n",
    "        test_losses[epoch] = test_loss\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs}: Train Loss: {train_loss:.2f}, Test Loss: {test_loss:.2f}, \\\n",
    "        Train Accuracy: {train_accuracy:.2f}, Test Accuracy: {test_accuracy:.2f},')\n",
    "        \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a379a2-1797-4ea6-8698-15837c1db018",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_losses = batch_gd(model, criterion, optimizer, train_dataloader, test_dataloader, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a44ab2c-8103-4adc-b090-3c2cf36a7672",
   "metadata": {},
   "source": [
    "### Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24cf9e-2f3e-454f-b668-2994bf189389",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Dwz9DT1qMuiY",
    "outputId": "a39a6e28-748b-4657-f7cc-405252298ddc"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Losess\")\n",
    "plt.plot(train_losses, label=\"Train loss\")\n",
    "plt.plot(test_losses, label=\"Test loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27909bc7-6ff4-4fdf-9623-94cfc5db39f8",
   "metadata": {},
   "source": [
    "### Predection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d153c98-8a73-45b8-9087-04cd378897b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predections = torch.max(outputs, 1)\n",
    "        \n",
    "        y_pred_list.append(targets.cpu().numpy())\n",
    "        y_true_list.append(predections.cpu().numpy())\n",
    "        \n",
    "targets = torch.tensor(np.concatenate(y_true_list))\n",
    "preds = torch.tensor(np.concatenate(y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f790ca62-12b3-40ce-99f5-b9c2dd1d6f88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfdfc1e-0b6e-46c3-944f-438bbdeb112e",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4764c-ccfd-4623-a4fc-b4ab85a3ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat = ConfusionMatrix(num_classes=K)\n",
    "cm = confmat(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3b670-5d22-42c8-9b35-9d920ede36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.heatmap(cm, annot=True, fmt='.0f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b450f5e8-285e-420f-bafc-81fff8b442f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = MulticlassAccuracy().to(device)\n",
    "accuracy = accuracy(preds, targets, num_classes=K)\n",
    "print(f'Accuracy: {100 * accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e1735e-6f8e-4553-aaf9-050129634759",
   "metadata": {},
   "source": [
    "### Precision & Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2b588-00b8-4a59-a4a3-3a182a5ac26c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision, recall = precision_recall(preds, targets, average='macro', num_classes=K)\n",
    "print(f'Precision: {100 * precision:.2f}%, Recall: {100 * recall:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3428a6-dfa3-4fa6-a048-68b111601e28",
   "metadata": {},
   "source": [
    "### F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21df2aa-bedb-4ee0-b95c-71bb7b3f8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = f1_score(preds, targets, num_classes=K)\n",
    "print(f'F1-score: {100 * f1_score:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
